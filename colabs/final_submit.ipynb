{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-submit.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/davis-contest/blob/main/colabs/final_submit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP4x9yydSFmb"
      },
      "source": [
        "![header](https://i.imgur.com/sAPM7Yy.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10csSKMe-ALo"
      },
      "source": [
        "# Submitting Final Results for DAVIS Contest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI_rGXvdSJQE"
      },
      "source": [
        "Starting at midnight Pacific time on March 29th,\n",
        "the test data (without labels) for the DAVIS contest will be made available.\n",
        "In order to make a submission to the contest for final evaluation,\n",
        "you will need to apply your model to that data\n",
        "and upload the outputs within 72 hours -- by 11:59 pm Pacific time on March 31st.\n",
        "\n",
        "This notebook includes instructions and example code for this submission process,\n",
        "demonstrated on the `DummyModel` trained in the\n",
        "[PyTorch starter notebook](https://wandb.me/davis-starter-pt).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNCu4YTvhcis"
      },
      "source": [
        "**End-to-end, the submission process is**:\n",
        "1. Download the test data from W&B\n",
        "1. Apply your model to the test data and upload results to W&B\n",
        "1. Submit your results at the [new leaderboard](https://wandb.ai/wandb/davis-contest-test/benchmark/leaderboard)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tn3qWvosTSu"
      },
      "source": [
        "**NOTE**: Before the test set is made available,\n",
        "the data downloaded by the code below\n",
        "is a _randomized_ version of the actual test set.\n",
        "This dataset has the same structure (folder names, file sizes, etc.)\n",
        "as the real test set, but the image files contain random bits.\n",
        "This allows you to check your submission pipeline for bugs and issues before the\n",
        "test data is released.\n",
        "Once the test data is released, the same code will download that data instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YSMzTGs_0Og"
      },
      "source": [
        "%%capture\n",
        "!pip install wandb==0.10.23\n",
        "!pip install git+https://github.com/wandb/davis-contest#egg=contest[torch,keras]\n",
        "!apt install tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkcREWQ5AFWb"
      },
      "source": [
        "import wandb\n",
        "\n",
        "import contest\n",
        "from contest.utils import paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC7t6oQm93pX"
      },
      "source": [
        "# Download Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC8F_5_Z6n0a"
      },
      "source": [
        "!wandb login --relogin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95EYWjIPoQKD"
      },
      "source": [
        "The test data,\n",
        "consisting of over 2000 frames across 22 video clips,\n",
        "is provided as a W&B Artifact,\n",
        "just like the training and validation data.\n",
        "\n",
        "These cells download that test data from [wandb.ai](https://wandb.ai).\n",
        "\n",
        "Note that only an obfuscated/randomized version of the test data\n",
        "is available before the test phase begins.\n",
        "\n",
        "Once the test phase is begins, the exact same code will\n",
        "download the correct version of the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68I7Cgmq_m7i"
      },
      "source": [
        "test_data_artifact_name = \"/\".join([\"wandb\", \"davis\", \"davis-contest-test\"])\n",
        "# once the real test set is released, the \"latest\" tag will point to it\n",
        "tag = \"latest\"\n",
        "\n",
        "test_data_artifact_id = test_data_artifact_name + \":\" + tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyRovUC_-q56"
      },
      "source": [
        "with wandb.init(project=\"davis\", job_type=\"download_data\") as run:\n",
        "  test_data_artifact = run.use_artifact(test_data_artifact_id)\n",
        "  test_data_path = test_data_artifact.download()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qkqe_hrJ976J"
      },
      "source": [
        "# Apply Model to Data and Upload Results to W&B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHVIRMnUokPZ"
      },
      "source": [
        "As during the earlier phase of the contest,\n",
        "results are submitted by\n",
        "1. generating model outputs and logging them to a W&B run, then\n",
        "2. submitting that run to a W&B Benchmark.\n",
        "\n",
        "Unlike during that phase,\n",
        "participants don't have access to the ground truth segmentations,\n",
        "and so won't be submitting their scores directly.\n",
        "Instead, results will be scored against a private ground truth.\n",
        "This helps ensure that the actual generalization performance of the model\n",
        "is being assessed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuCDNKWdBSrl"
      },
      "source": [
        "####\n",
        "## REPLACE THIS CELL WITH YOUR MODEL SETUP CODE\n",
        "####\n",
        "\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "class DummyModel(pl.LightningModule):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv = torch.nn.Conv2d(in_channels=3, out_channels=1, kernel_size=1)\n",
        "\n",
        "  def forward(self, xs):\n",
        "    return torch.sigmoid(self.conv(xs))\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    loss = self.forward_on_batch(batch)\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    loss = self.forward_on_batch(batch)\n",
        "    return loss\n",
        "\n",
        "  def forward_on_batch(self, batch):\n",
        "    xs, ys = batch\n",
        "    y_hats = self.forward(xs)\n",
        "    loss = F.binary_cross_entropy(y_hats, ys)\n",
        "    return loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return torch.optim.SGD(self.parameters(), lr=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAsxw78-WeNP"
      },
      "source": [
        "The following code cell shows how to execute your model on the test set\n",
        "and generate your results as an artifact for submission.\n",
        "\n",
        "It is based on the code in the section \"Run Your Model on the Evaluation Data\" of the starter notebooks\n",
        "([PyTorch Lightning](https://colab.research.google.com/github/wandb/davis-contest/blob/main/colabs/starter_torch.ipynb#scrollTo=y7gj0eF8yc2S),\n",
        "[Keras](https://wandb.me/davis-starter-keras)),\n",
        "which ran models on the validation data.\n",
        "\n",
        "The key difference is that here,\n",
        "the data being used is from the test set,\n",
        "rather than the validation set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPT4BQg-tvBv"
      },
      "source": [
        "\n",
        "Because the data is in the same format\n",
        "(described in the\n",
        "[starter notebooks](https://colab.research.google.com/github/wandb/davis-contest/blob/main/colabs/starter_torch.ipynb#scrollTo=aw9pYa8vxEGJ))\n",
        "as the validation set,\n",
        "you should be able to use the exact same pipeline you used to submit your results on the validation set to submit your results on the test set.\n",
        "\n",
        "In particular, once you've downloaded the test data\n",
        "and constructed a `pd.DataFrame` of paths\n",
        "(see code snippet below),\n",
        "the remainder of the evaluation process\n",
        "should proceed exactly as on the validation data.\n",
        "```python\n",
        "with wandb.init(project=\"davis\", job_type=\"run-test\", save_code=True) as run:\n",
        "  test_data_artifact = run.use_artifact(test_data_artifact_id)\n",
        "  test_data_paths = paths.artifact_paths(test_data_artifact)\n",
        "```\n",
        "\n",
        "As during validation,\n",
        "the result is a W&B Artifact with a particular structure,\n",
        "described [in the starter notebooks](https://colab.research.google.com/github/wandb/davis-contest/blob/main/colabs/starter_torch.ipynb#scrollTo=pKYX3uWwPfWW). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVY_KkMqts4z"
      },
      "source": [
        "\n",
        "If you run into any issues, reach out to the contest support in any of these ways:\n",
        "- post in `#qualcomm-competition` on the Weights & Biases [Slack forum](https://wandb.me/slack),\n",
        "- email `support@wandb.com`, or\n",
        "- message W&B tech support via the grey speech bubble icon on [wandb.ai](https://wandb.ai) (bottom right-hand corner)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmTcPlYKCSm9"
      },
      "source": [
        "output_dir = \"outputs\"\n",
        "!rm -rf {output_dir}\n",
        "!mkdir -p {output_dir}\n",
        "\n",
        "result_artifact_name = \"test-result\"\n",
        "\n",
        "with wandb.init(project=\"davis\", job_type=\"run-test\") as run:\n",
        "\n",
        "  test_data_artifact = run.use_artifact(test_data_artifact_id)\n",
        "  test_data_paths = paths.artifact_paths(test_data_artifact)\n",
        "\n",
        "  ####\n",
        "  ## REPLACE THIS WITH YOUR DATA LOADING CODE, IF NEEDED\n",
        "  ####\n",
        "  test_dataset = contest.torch.data.VidSegDataset(\n",
        "    test_data_paths, has_annotations=False)\n",
        "  num_images = len(test_dataset)\n",
        "  test_dataloader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=1)\n",
        "\n",
        "  ####\n",
        "  ## REPLACE THIS WITH YOUR MODEL LOADING CODE\n",
        "  ####\n",
        "  model_artifact_name = \"charlesfrye/davis/dummy-baseline\" \n",
        "  model_tag = \"latest\"\n",
        "  model = contest.torch.utils.load_model_from_artifact(\n",
        "    model_artifact_name + \":\" + model_tag, DummyModel) \n",
        "\n",
        "  ## If using Keras, see starter notebook for\n",
        "  ##   code to print and count params of your model\n",
        "\n",
        "  # print a summary of the model\n",
        "  print(model)\n",
        "\n",
        "  # the number of parameters in the model must be logged\n",
        "  print(\"\\n\")\n",
        "  nparams = contest.torch.profile.count_params(model)\n",
        "\n",
        "  profiling_info = {\"nparams\": nparams}\n",
        "  wandb.log(profiling_info)\n",
        "\n",
        "  ####\n",
        "  ## REPLACE THIS WITH YOUR MODEL EXECUTION CODE, IF NEEDED\n",
        "  ####\n",
        "  output_paths = contest.torch.evaluate.run(\n",
        "    model, test_dataloader, num_images, output_dir)\n",
        "\n",
        "  # the number of parameters in the model should also be included in the result artifact\n",
        "  result_artifact = contest.evaluate.make_result_artifact(\n",
        "    output_paths, result_artifact_name + \"-\" + run.entity, metadata=profiling_info)\n",
        "  run.log_artifact(result_artifact)\n",
        "\n",
        "  run_path = run.path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L9qDhP_-lJl"
      },
      "source": [
        "# Submit to the Final Leaderboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lWLRG-CiWyz"
      },
      "source": [
        "Once you've logged your results to W&B,\n",
        "you can officially submit the run to the\n",
        "final leaderboard on the\n",
        "[Weights & Biases benchmark for the test dataset](http://wandb.me/davis-test-benchmark).\n",
        "\n",
        "This works much like submission for results on the validation set,\n",
        "except that your run doesn't have a `segmentation_metric` score --\n",
        "that'll be calculated behind the scenes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvFnZrDQrgT1"
      },
      "source": [
        "To submit, click the \"Submit a run\" tab on the right side of the \n",
        "[benchmark page](https://wandb.me/davis-test-benchmark)\n",
        "and then paste or type in the\n",
        "W&B run path for the run\n",
        "you used to score your submission.\n",
        "Click the \"Submit\" button\n",
        "to start the submission process.\n",
        "\n",
        "Submissions will be manually reviewed.\n",
        "Results submitted before the deadline but only reviewed after it\n",
        "are still eligible.\n",
        "\n",
        "If you ran the cells above to execute your run, the run path will be printed by the cell below.\n",
        "\n",
        "See the instructions on the\n",
        "\"Submit a run\" tab\n",
        "for more on run paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuA0MWaKry9D"
      },
      "source": [
        "run_path"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}