{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Using a Pre-Trained ConvNet to Segment Videos",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRzwwDDjTSOM"
      },
      "source": [
        "![header](https://i.imgur.com/sAPM7Yy.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAIUGkl3nrzI"
      },
      "source": [
        "# Learning to Segment Videos from Pre-Computed Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzalmuMqn256"
      },
      "source": [
        "Segmenting videos is challenging,\r\n",
        "and even though the DAVIS dataset is nearly half a gigabyte in size,\r\n",
        "it's not nearly big enough for a neural network to learn\r\n",
        "everything about video segmentation from scratch.\r\n",
        "\r\n",
        "In order to succeed, you'll need to take advantage of _pre-training_:\r\n",
        "letting the network learn part of a task from a larger dataset\r\n",
        "that demonstrates a related task,\r\n",
        "and then learning the rest of the task from a smaller dataset.\r\n",
        "\r\n",
        "Pre-training is ubiquitous in human and animal learning.\r\n",
        "Before learning to read sheet music,\r\n",
        "we often first learn to read natural language.\r\n",
        "Before learning to read a language,\r\n",
        "we learn related things:\r\n",
        "how to speak that language,\r\n",
        "how to recognize objects and symbols.\r\n",
        "This speeds up our learning immensely.\r\n",
        "We'd like to do the same for our neural network,\r\n",
        "showing it some related data that will help it\r\n",
        "learn to segment videos.\r\n",
        "\r\n",
        "In particular, there are lots of great datasets of images out there,\r\n",
        "and even more neural networks trained on those datasets.\r\n",
        "These networks already know a lot about images,\r\n",
        "which overlaps quite substantially with what they need to learn about videos\r\n",
        "in order to segment them.\r\n",
        "\r\n",
        "In this notebook,\r\n",
        "we'll work through how to apply a pretrained image model,\r\n",
        "[AlexNet](https://arxiv.org/abs/1404.5997),\r\n",
        "to the video segmentation problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scbJLqwrWrHC"
      },
      "source": [
        "%%capture\n",
        "!pip install \"git+https://www.github.com/wandb/davis-contest.git#egg=contest[torch]\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7oYSvKggRAJ"
      },
      "source": [
        "from functools import lru_cache\r\n",
        "import os\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pytorch_lightning as pl\r\n",
        "import skimage.io\r\n",
        "import torch\r\n",
        "import torchvision.models as models\r\n",
        "import wandb\r\n",
        "\r\n",
        "import contest\r\n",
        "from contest.utils import clips, paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdg3la8kpK3h"
      },
      "source": [
        "# Working with a Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vfj6CHaoobk"
      },
      "source": [
        "The basic idea is that the pre-trained model extracts _features_,\r\n",
        "a representation of the inputs that pulls out the useful information\r\n",
        "from the cacophony of pixels.\r\n",
        "In convolutional networks,\r\n",
        "these features are detected throughout the entire image,\r\n",
        "creating a _feature map_,\r\n",
        "a value of the feature for each spatial location.\r\n",
        "\r\n",
        "For example, one feature might be \"contains a dog\":\r\n",
        "each pixel value in this feature map is large where the associated\r\n",
        "region of the image appears, to the network,\r\n",
        "to contain a dog.\r\n",
        "\r\n",
        "For more on the features extracted by convolutional networks,\r\n",
        "see [this paper from the Distill project](https://distill.pub/2017/feature-visualization/)\r\n",
        "or dive deeper with the\r\n",
        "[Circuits series from OpenAI](https://distill.pub/2020/circuits/).\r\n",
        "\r\n",
        "From these features -- which are much smaller than the input frame --\r\n",
        "a separate neural network learns to build segmentations.\r\n",
        "For that smaller network, DAVIS can provide more than enough data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcy0EX7gpN0_"
      },
      "source": [
        "## Data Engineering\r\n",
        "\r\n",
        "Contemporary ML engineering projects\r\n",
        "generally have two separate components:\r\n",
        "\r\n",
        "1) a _data engineering_ component,\r\n",
        "which involves fetching data\r\n",
        "and getting it into the GPU,\r\n",
        "and\r\n",
        "\r\n",
        "2) a _model engineering component_,\r\n",
        "which involves building a model\r\n",
        "that consumes that data\r\n",
        "through training.\r\n",
        "\r\n",
        "Getting both pieces right\r\n",
        "is critical for a successful ML project.\r\n",
        "Below, we dive into how pre-training\r\n",
        "impacts each of these components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89pszBDOVX2H"
      },
      "source": [
        "\r\n",
        "### Why Use Precomputed Features? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LdNxKXCYHfC"
      },
      "source": [
        "Two major bottlenecks for the data engineering pipeline\r\n",
        "are reading data from disk\r\n",
        "and transferring data from CPU RAM to GPU RAM.\r\n",
        "\r\n",
        "We can get around both bottlenecks by loading the entirety\r\n",
        "of our dataset into GPU RAM at once.\r\n",
        "\r\n",
        "But GPU RAM also needs to hold our model,\r\n",
        "its intermediate computations\r\n",
        "and its gradients,\r\n",
        "which means space is at a premium.\r\n",
        "\r\n",
        "We can save a tremendous amount of space\r\n",
        "by recognizing that, for training,\r\n",
        "we don't actually need the videos themselves.\r\n",
        "We aren't training the network that computes the features,\r\n",
        "and so those features will be fixed throughout training.\r\n",
        "\r\n",
        "So let's instead treat the features as the input data,\r\n",
        "rather than videos.\r\n",
        "They're much smaller and so will fit comfortably in memory\r\n",
        "with everything else.\r\n",
        "\r\n",
        "This process is known as _feature extraction_\r\n",
        "or using _precomputed features_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2XhkSnwaPX2"
      },
      "source": [
        "The code below constructs a `Dataset` object\r\n",
        "and an associated `LightningDataModule`\r\n",
        "that apply a `featurizer` network to their inputs.\r\n",
        "\r\n",
        "For more on `LightningDataModule`s and\r\n",
        "data engineering in PyTorch,\r\n",
        "see [this video](https://www.youtube.com/watch?v=L---MBeSXFw)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkbt6xMIbdxw"
      },
      "source": [
        "class FeaturizedDataset(torch.utils.data.Dataset):\r\n",
        "\r\n",
        "  def __init__(self, featurized_xs, paths_df=None, mask_transform=None):\r\n",
        "\r\n",
        "    self.featurized_xs = featurized_xs\r\n",
        "\r\n",
        "    self.paths_df = paths_df\r\n",
        "    if self.paths_df is not None:\r\n",
        "      self.annotation_paths = self.paths_df[\"annotation\"]\r\n",
        "    else:\r\n",
        "      self.annotation_paths is None\r\n",
        "    self.mask_transform = mask_transform\r\n",
        "\r\n",
        "    self.len = len(self.featurized_xs)\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return self.len\r\n",
        "\r\n",
        "  @lru_cache(maxsize=None)\r\n",
        "  def __getitem__(self, idx):\r\n",
        "    x = self.featurized_xs[idx]\r\n",
        "\r\n",
        "    if torch.is_tensor(idx):\r\n",
        "      idx = idx.to_list()\r\n",
        "\r\n",
        "    if self.annotation_paths is None:\r\n",
        "      return x\r\n",
        "    else:\r\n",
        "      annotation_name = self.annotation_paths.iloc[idx]\r\n",
        "      annotation = skimage.io.imread(annotation_name)\r\n",
        "      if self.mask_transform is not None:\r\n",
        "        annotation = self.mask_transform(annotation)\r\n",
        "  \r\n",
        "      return x, annotation\r\n",
        "\r\n",
        "  @staticmethod\r\n",
        "  def _apply_featurizer(featurizer, dataloader):\r\n",
        "    featurized_xs = []\r\n",
        "    for batch in dataloader:\r\n",
        "      xs, ys = batch\r\n",
        "      featurized_xs.append(featurizer.forward(xs))\r\n",
        "    featurized_xs = torch.cat(featurized_xs)\r\n",
        "\r\n",
        "    return featurized_xs\r\n",
        "\r\n",
        "\r\n",
        "  @classmethod\r\n",
        "  def from_raw_data(cls, featurizer, raw_dataloader,\r\n",
        "                    paths_df=None, mask_transform=None):\r\n",
        "    featurized_xs = FeaturizedDataset._apply_featurizer(featurizer, raw_dataloader)\r\n",
        "    return cls(featurized_xs, paths_df, mask_transform)\r\n",
        "\r\n",
        "\r\n",
        "class FeaturizedDataModule(pl.LightningDataModule):\r\n",
        "\r\n",
        "  def __init__(self, featurizer, paths_df, has_annotations=True, num_workers=0,\r\n",
        "               image_transform=None, mask_transform=None, batch_size=None,\r\n",
        "               featurizer_batch_size=None):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    self.paths_df = paths_df\r\n",
        "    self.has_annotations = has_annotations\r\n",
        "    self.num_workers = num_workers \r\n",
        "    self.featurizer = featurizer\r\n",
        "\r\n",
        "    if image_transform is None:\r\n",
        "      self.image_transform = contest.torch.data.default_image_transform\r\n",
        "    else:\r\n",
        "      self.image_transform = self.image_transform\r\n",
        "    \r\n",
        "    if mask_transform is None:\r\n",
        "      self.mask_transform = contest.torch.data.default_mask_transform\r\n",
        "    else:\r\n",
        "      self.mask_transform = self.mask_transform\r\n",
        "\r\n",
        "    if batch_size is None:\r\n",
        "      self.batch_size = len(paths_df)\r\n",
        "    else:\r\n",
        "      self.batch_size = batch_size\r\n",
        "\r\n",
        "    if featurizer_batch_size is None:\r\n",
        "      self.featurizer_batch_size = 32\r\n",
        "    else:\r\n",
        "      self.featurizer_batch_size = featurizer_batch_size\r\n",
        "\r\n",
        "  def setup(self, stage=None):\r\n",
        "      self.raw_dataset = contest.torch.data.VidSegDataset(\r\n",
        "        self.paths_df, self.has_annotations,\r\n",
        "        image_transform=self.image_transform,\r\n",
        "        mask_transform=self.mask_transform)\r\n",
        "      self.raw_dataloader = torch.utils.data.DataLoader(\r\n",
        "          self.raw_dataset, batch_size=self.featurizer_batch_size,\r\n",
        "          num_workers=self.num_workers)\r\n",
        "\r\n",
        "      self.featurized_dataset = FeaturizedDataset.from_raw_data(\r\n",
        "        self.featurizer, self.raw_dataloader, self.paths_df,\r\n",
        "        mask_transform=self.mask_transform)\r\n",
        "\r\n",
        "  def prepare_data(self, stage=None):\r\n",
        "    if stage == \"fit\" or stage is None:\r\n",
        "      self.train_dataset = self.featurized_dataset\r\n",
        "\r\n",
        "  def train_dataloader(self):\r\n",
        "    return torch.utils.data.DataLoader(self.featurized_dataset, batch_size=self.batch_size,\r\n",
        "                                       num_workers=self.num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVjtBrongsK-"
      },
      "source": [
        "### Applying the Pretrained Model to the Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaMYdSk8eZ1-"
      },
      "source": [
        "The deep learning community has developed numerous tools\r\n",
        "for sharing and distributing pre-trained models.\r\n",
        "\r\n",
        "Specifically for computer vision, the\r\n",
        "[`torchvision.models`](https://pytorch.org/vision/0.8/models.html)\r\n",
        "module provides easy access to a variety of\r\n",
        "widely-used and performant pre-trained convolutional neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_2fl35nhD1p"
      },
      "source": [
        "def get_alexnet():\r\n",
        "  alexnet = models.alexnet(pretrained=True)\r\n",
        "  alexnet.eval = True    \r\n",
        "  for param in alexnet.parameters():\r\n",
        "      param.requires_grad = False\r\n",
        "      \r\n",
        "  featurizer = alexnet.features\r\n",
        "  return featurizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA4pNmPme0v2"
      },
      "source": [
        "In order to apply the pre-trained model to the training data,\r\n",
        "we need the training data.\r\n",
        "\r\n",
        "The training data for the contest is stored and distributed using\r\n",
        "Weights & Biases [Artifacts](https://docs.wandb.ai/artifacts/api).\r\n",
        "For more on using Artifacts, see the\r\n",
        "[starter colabs](https://github.com/wandb/davis-contest/tree/main/colabs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-vFDxn6gpVi"
      },
      "source": [
        "# picking out the training data artifact by name\r\n",
        "\r\n",
        "entity = \"wandb\"  # artifacts are associated with an entity -- a user or team\r\n",
        "project = \"davis\"  # artifacts are associated with a project -- a collection of ML experiments\r\n",
        "split = \"train\"  # the train and val data are both stored in the same format\r\n",
        "tag = \"contest\"  # different versions of an Artifact have different tags\r\n",
        "\r\n",
        "training_data_artifact_id = os.path.join(entity, project, f\"davis2016-{split}\") + \":\" + tag\r\n",
        "training_data_artifact_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgj0OWTJi1Sm"
      },
      "source": [
        "def apply_featurization(data_artifact_id, featurizer, output_artifact_name):\r\n",
        "  if featurizer == \"alexnet\":\r\n",
        "    featurizer = get_alexnet()\r\n",
        "  else:\r\n",
        "    raise ValueError(f\"unknown featurizer: {featurizer}\")\r\n",
        "\r\n",
        "  data_artifact = run.use_artifact(data_artifact_id)\r\n",
        "  paths_df = paths.artifact_paths(data_artifact)\r\n",
        "\r\n",
        "  fdm = FeaturizedDataModule(featurizer, paths_df)\r\n",
        "  fdm.setup()\r\n",
        "\r\n",
        "  output_artifact = wandb.Artifact(output_artifact_name,\r\n",
        "                                   type=\"featurized-data\")\r\n",
        "\r\n",
        "  featurized_array = fdm.featurized_dataset.featurized_xs.numpy()\r\n",
        "  np.save(\"features.npy\", featurized_array)\r\n",
        "  output_artifact.add_file(\"features.npy\", \"features_array\")\r\n",
        "\r\n",
        "  paths_df.to_json(\"paths.json\")\r\n",
        "  output_artifact.add_file(\"paths.json\")\r\n",
        "\r\n",
        "  wandb.run.log_artifact(output_artifact)\r\n",
        "\r\n",
        "  try:\r\n",
        "    output_artifact.wait()\r\n",
        "  except AttributeError:\r\n",
        "    pass\r\n",
        "\r\n",
        "\r\n",
        "  return \"/\".join([wandb.run.entity, wandb.run.project, output_artifact.name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i88s2TWKgzOd"
      },
      "source": [
        "config = {\"featurizer\": \"alexnet\"}\r\n",
        "\r\n",
        "with wandb.init(project=project, job_type=\"featurize\", config=config) as run:\r\n",
        "\r\n",
        "  output_artifact_name = f\"{wandb.config['featurizer']}-featurized-train\"\r\n",
        "\r\n",
        "  output_artifact_id = apply_featurization(\r\n",
        "    training_data_artifact_id, wandb.config[\"featurizer\"], output_artifact_name) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FWoEKwwaScd"
      },
      "source": [
        "### Loading Featurized Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SspCB2fcHRA"
      },
      "source": [
        "Now that we've precomputed the features,\r\n",
        "we don't need to keep the original data around.\r\n",
        "\r\n",
        "The cell below defines\r\n",
        "a new `LightningDataModule` and `Dataset`\r\n",
        "that make use of the saved precomputed features from above,\r\n",
        "rather than working from the original data.\r\n",
        "\r\n",
        "We'll use these below in our training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC-ynfe1WrHN"
      },
      "source": [
        "class PrecomputedFeaturesDataModule(pl.LightningDataModule):\n",
        "\n",
        "  def __init__(self, features_file, annotation_files=None, batch_size=32):\n",
        "    self.batch_size = batch_size\n",
        "    self.features_file = features_file\n",
        "    self.annotation_files = annotation_files\n",
        "\n",
        "  def setup(self):\n",
        "    self.dataset = PrecomputedFeaturesDataset(\n",
        "      self.features_file, self.annotation_files)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return torch.utils.data.DataLoader(self.dataset, batch_size=self.batch_size)\n",
        "\n",
        "\n",
        "class PrecomputedFeaturesDataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, features_file, annotation_files=None, mask_transform=None):\n",
        "    self.features_file = features_file\n",
        "    self.annotation_files = annotation_files\n",
        "    if mask_transform is None:\n",
        "      mask_transform = contest.torch.data.default_mask_transform\n",
        "    self.mask_transform = mask_transform\n",
        "\n",
        "    self.load_features(self.features_file)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.featurized_xs)\n",
        "\n",
        "  @lru_cache(maxsize=None)\n",
        "  def __getitem__(self, idx):\n",
        "    x = self.featurized_xs[idx]\n",
        "\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.to_list()\n",
        "    if self.annotation_files is None:\n",
        "      return x\n",
        "    else:\n",
        "      annotation_name = self.annotation_files.iloc[idx]\n",
        "      annotation = skimage.io.imread(annotation_name)\n",
        "      annotation = self.mask_transform(annotation)\n",
        "\n",
        "      return x, annotation\n",
        "\n",
        "  def load_features(self, features_file):\n",
        "    self.featurized_xs = torch.Tensor(np.load(features_file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWDD7Xhsqgd7"
      },
      "source": [
        "## Model Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B08k7X-aqiXL"
      },
      "source": [
        "With our big network doing most of the work for us,\r\n",
        "we can get pretty good performance without doing too much ourselves.\r\n",
        "\r\n",
        "Here, we build the simplest possible network on top:\r\n",
        "a single linear (convolutional) layer,\r\n",
        "followed by a `sigmoid` function so that the results are scaled appropriately.\r\n",
        "\r\n",
        "We use the `B`inary `C`ross `E`ntropy `Loss` function,\r\n",
        "which penalizes the network especially heavily for confidently segmenting\r\n",
        "areas where there is no subject in the ground truth.\r\n",
        "\r\n",
        "You might try others!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2getMIqY3oEU"
      },
      "source": [
        "model_name = \"simple-decoder\"\n",
        "\n",
        "class SimpleDecoder(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, target_size=(480, 854)):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv = torch.nn.Conv2d(256, 1, kernel_size=3)\n",
        "    self.resize = torch.nn.AdaptiveAvgPool2d(target_size)\n",
        "    self.cost = torch.nn.BCELoss()\n",
        "\n",
        "  def forward(self, xs):\n",
        "    xs = self.conv(xs)\n",
        "    xs = torch.sigmoid(xs)\n",
        "    return self.resize(xs)\n",
        "\n",
        "  def loss(self, outs, ys):\n",
        "    return self.cost(outs, ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCJd7-FnrS9K"
      },
      "source": [
        "### Training and Logging with Weights & Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaWd6J0srY7P"
      },
      "source": [
        "In the cell below, we define a `train`ing function\r\n",
        "that glues together our precomputed features and our `Simple` model.\r\n",
        "\r\n",
        "Included are some Weights & Biases logging tools:\r\n",
        "in particular, tracking the predictions and the ground truth\r\n",
        "so that we can look at how the network's outputs compare to the correct answers\r\n",
        "and how they develop during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8i8zRHoWrHN"
      },
      "source": [
        "def train(model, optimizer, dataloader, model_name, steps=1, log_freq=10, device=\"cuda\",\n",
        "          run=None):\n",
        "\n",
        "  if run is None:\n",
        "    run = wandb.init(project=\"davis\", job_type=\"train\")\n",
        "\n",
        "  model.train = True\n",
        "  model.to(device)\n",
        "  \n",
        "  model_artifact = wandb.Artifact(model_name, type=\"trained-model\",\n",
        "                                  metadata={})\n",
        "\n",
        "  class_labels = {0: \"background\", 1: \"object\"}\n",
        "  dataiterator = iter(dataloader)\n",
        "\n",
        "  for step in range(steps):\n",
        "    try:\n",
        "      xs, ys = next(dataiterator)\n",
        "    except StopIteration:\n",
        "      dataiterator = iter(dataloader)\n",
        "      xs, ys = next(dataiterator)\n",
        "    xs, ys = xs.to(device), ys.to(device)\n",
        "      \n",
        "    outs = model(xs)\n",
        "    loss = model.loss(outs, ys)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if not step % log_freq:\n",
        "      img, out, target = (xs[0].detach().to(\"cpu\"),\n",
        "                          outs[0].detach().to(\"cpu\"),\n",
        "                          ys[0].detach().to(\"cpu\"))\n",
        "      mask = torch.round(out)\n",
        "      img = img.permute(1, 2, 0)\n",
        "      mask = torch.squeeze(mask)\n",
        "      target = torch.squeeze(target)\n",
        "      img = target  # for featurized models\n",
        "\n",
        "      mask_img = wandb.Image(img.numpy(), masks={\n",
        "          \"predictions\": {\n",
        "              \"mask_data\": mask.numpy(),\n",
        "              \"class_labels\": class_labels\n",
        "          },\n",
        "          \"ground_truth\": {\n",
        "              \"mask_data\": target.numpy(),\n",
        "              \"class_labels\": class_labels\n",
        "          }\n",
        "      })\n",
        "      wandb.log({\"loss\": float(loss),\n",
        "                  \"prediction\": mask_img},\n",
        "                step=step)\n",
        "\n",
        "      filename = f\"model-{str(step).zfill(8)}.pt\"\n",
        "      torch.save(model.state_dict(), filename)\n",
        "\n",
        "      model_artifact.add_file(filename)\n",
        "\n",
        "  model_artifact.add_file(filename, \"final_model\")\n",
        "\n",
        "  run.log_artifact(model_artifact)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE7PTJq-4GxB"
      },
      "source": [
        "config = {\"batch_size\": 32,\r\n",
        "          \"lr\": 5e-4,\r\n",
        "          \"betas\": (0.9, 0.999),\r\n",
        "          \"steps\": 101,\r\n",
        "          \"log_freq\": 10,\r\n",
        "          \"featurizer\": \"alexnet\"}\r\n",
        "\r\n",
        "\r\n",
        "with wandb.init(project=\"davis\", job_type=\"train\", config=config) as run:\r\n",
        "\r\n",
        "  featurized_artifact_id = f\"davis/{wandb.config['featurizer']}-featurized-train:latest\"\r\n",
        "  precomputed_features = run.use_artifact(featurized_artifact_id)\r\n",
        "  precomputed_features_dir = precomputed_features.download()\r\n",
        "  precomputed_features_path = os.path.join(precomputed_features_dir, \"features_array\")\r\n",
        "\r\n",
        "  raw_data_artifact = run.use_artifact(training_data_artifact_id)\r\n",
        "  raw_paths_df = paths.artifact_paths(raw_data_artifact)\r\n",
        "\r\n",
        "  pcfdm = PrecomputedFeaturesDataModule(precomputed_features_path,\r\n",
        "                                        raw_paths_df[\"annotation\"],\r\n",
        "                                        batch_size=wandb.config[\"batch_size\"])\r\n",
        "  pcfdm.setup()\r\n",
        "  tdl = pcfdm.train_dataloader()\r\n",
        "\r\n",
        "  model = SimpleDecoder()\r\n",
        "  wandb.watch(model, log_freq=wandb.config[\"log_freq\"])\r\n",
        "  optimizer = torch.optim.Adam(model.parameters(),\r\n",
        "                               lr=wandb.config[\"lr\"],\r\n",
        "                               betas=wandb.config[\"betas\"])\r\n",
        "\r\n",
        "  train(model, optimizer, tdl, model_name,\r\n",
        "        steps=wandb.config[\"steps\"], log_freq=wandb.config[\"log_freq\"],\r\n",
        "        run=run)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owJllNQkM7l9"
      },
      "source": [
        "# Packaging Results for Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_yHP7rss17e"
      },
      "source": [
        "Submissions to the contest need to be put into a particular format in order to be considered and evaluated.\r\n",
        "\r\n",
        "Below, we'll package up the results of our pre-trained model\r\n",
        "into this format.\r\n",
        "\r\n",
        "See the [contest instructions](https://github.com/wandb/davis-contest)\r\n",
        "and the [starter notebooks](https://github.com/wandb/davis-contest/tree/main/colabs)\r\n",
        "for more details on this format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pGU_maalz6g"
      },
      "source": [
        "split = \"val\"\r\n",
        "validation_data_artifact_id = \"/\".join([entity, project, f\"davis2016-{split}\"]) + \":\" + tag\r\n",
        "validation_data_artifact_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9moub7T2Iy1"
      },
      "source": [
        "## Featurizing the Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHOZS19QlrrU"
      },
      "source": [
        "config = {\"featurizer\": \"alexnet\"}\r\n",
        "\r\n",
        "with wandb.init(project=project, job_type=\"featurize\", config=config) as run:\r\n",
        "\r\n",
        "  output_artifact_name = f\"{wandb.config['featurizer']}-featurized-val\"\r\n",
        "\r\n",
        "  apply_featurization(\r\n",
        "    validation_data_artifact_id, wandb.config[\"featurizer\"], output_artifact_name) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4wY70ZA5JoP"
      },
      "source": [
        "## Running the Model on the Featurized Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWJjK1xcvJqg"
      },
      "source": [
        "model_artifact_id = f\"davis/{model_name}:latest\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJSzu2KfL-we"
      },
      "source": [
        "output_dir = os.path.join(\"outputs\")\r\n",
        "!rm -rf output_dir\r\n",
        "!mkdir -p {output_dir}\r\n",
        "\r\n",
        "result_artifact_name = model_name + \"-result\"\r\n",
        "\r\n",
        "config = {\"batch_size\": 32,\r\n",
        "          \"featurizer\": \"alexnet\",\r\n",
        "          \"model\": model_name}\r\n",
        "\r\n",
        "with wandb.init(project=\"davis\", job_type=\"run-val\", config=config) as run:\r\n",
        "\r\n",
        "  # get and set up data\r\n",
        "  featurized_artifact_id = f\"davis/{wandb.config['featurizer']}-featurized-val:latest\"\r\n",
        "  precomputed_features = run.use_artifact(featurized_artifact_id)\r\n",
        "  precomputed_features_dir = precomputed_features.download()\r\n",
        "  precomputed_features_path = os.path.join(precomputed_features_dir, \"features_array\")\r\n",
        "\r\n",
        "  raw_data_artifact = run.use_artifact(validation_data_artifact_id)\r\n",
        "  raw_paths_df = paths.artifact_paths(raw_data_artifact)\r\n",
        "\r\n",
        "  pcfdm = PrecomputedFeaturesDataModule(precomputed_features_path,\r\n",
        "                                        batch_size=wandb.config[\"batch_size\"])\r\n",
        "  pcfdm.setup()\r\n",
        "  tdl = pcfdm.train_dataloader()\r\n",
        "\r\n",
        "  # get and set up featurizer and model\r\n",
        "  if wandb.config[\"featurizer\"] == \"alexnet\":\r\n",
        "    featurizer = get_alexnet()\r\n",
        "  else:\r\n",
        "    raise ValueError(f\"unknown featurizer {wandb.config['featurizer']}\")\r\n",
        "  model = contest.torch.utils.load_model_from_artifact(model_artifact_id, SimpleDecoder)\r\n",
        "\r\n",
        "  # profiling metadata\r\n",
        "  ## don't forget to include the parameters from your featurizing model!\r\n",
        "  nparams = contest.torch.profile.count_params(featurizer) +\\\r\n",
        "            contest.torch.profile.count_params(model) \r\n",
        "\r\n",
        "  profiling_metadata = {\"nparams\": nparams}\r\n",
        "  wandb.log(profiling_metadata)\r\n",
        "\r\n",
        "  output_paths = contest.torch.evaluate.run(model, tdl, len(pcfdm.dataset), output_dir)\r\n",
        "\r\n",
        "  result_artifact = contest.evaluate.make_result_artifact(\r\n",
        "    output_paths, result_artifact_name, metadata=profiling_metadata\r\n",
        "  )\r\n",
        "\r\n",
        "  run.log_artifact(result_artifact)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}